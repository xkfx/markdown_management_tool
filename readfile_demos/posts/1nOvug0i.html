<div>
<p><strong style="font-size: 1.5em; line-height: 1.5;">Chapter 1</strong></p>
Interesting read, but you can skip it.<br /><br />
<h2>Chapter 2</h2>



<strong>2.1 </strong>Insertion
 Sort - To be honest you should probably know all major sorting 
algorithms, not just insertion sort. It's just basic knowledge and you 
never know when it can help.<br /><strong>2.2 </strong>Analysis of Algorithms - you can skip the small intro, but know the rest.<br /><strong>2.3 </strong>Designing
 algorithms - contains merge sort and its analysis as well as an 
overview of divide-and-conquer, very important stuff, so worth a read.<br /><br />
<h2>Chapter 3</h2>



All of it. You have to know big-O notation and time complexity analysis, period.<br /><br />
<h2>Chapter 4</h2>



<strong>4.1 </strong>Maximum
 subarray problem - Can kind of be worth your time. There are better 
solutions to this problem than divide and conquer but it's good practice
 and the flow of logic may help develop how you think.<br /><strong>4.2 </strong>Strassen's
 algorithm - I really love this algorithm and was astounded at how cool 
it was the first time I saw it, but you can skip it for the interviews. 
It won't come up.<br /><strong>4.3 </strong>Substitution method - you won't be using
 this method in an interview, but you should know it since it's a basic 
tool for finding the time complexity of a recursive algorithm.<br /><strong>4.4 </strong>Recurrence tree method - same as 4.3<br /><strong>4.5 </strong>Master
 method - essential knowledge. You should know it and practice with it 
and be able to use it in 3 seconds. This is the method you would use in 
an interview if analyzing a recursive algorithm that fits the form.<br /><strong>4.6 </strong>Proof
 of the master theorem - you can probably skip this, though it's good to
 read at least once so that you understand what you're doing with the 
master method.<br /><br />
<h2>Chapter 5</h2>



I've never read this chapter, to
 be honest, but what I know is that you need a basic grasp of 
probability in interviews because there's a good chance they may come 
up. That said, as long as you know basic probability concepts and 
practice on probability-related interview problems (there are such 
problems with solution explanations in <a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//www.amazon.com/Elements-Programming-Interviews-Insiders-Guide/dp/1479274836" rel="nofollow noreferrer" target="_blank">Elements of Programming Interviews</a>,
 the book I recommend for interview prep), you can probably skip this 
chapter. From a cursory glance, it's more math than algorithms.<br /><br />
<h2>Chapter 6</h2>



<strong>6.1, 6.2, 6.3, 6.4, 6.5</strong> - Heaps and heapsort. Check.<br /><br />
<h2>Chapter 7</h2>



<strong>7.1, 7.2, 7.3 - </strong>Quicksort
 and its randomized version. Need-to-know concepts. I also recommend 7.4
 (I was once asked in an interview to high-level-analyze a randomized 
algorithm), though the probability you have to deal with something like 
7.4 in an interview is pretty low, I'd guess.<br /><br />
<h2>Chapter 8</h2>



<strong>8.1 - </strong>Lower
 bounds on sorting - Yes. Basic knowledge. May be asked in a Google 
interview (though unlikely, I know of a case it happened in before).<br /><strong>8.2 - </strong>Counting sort - Need-to-know in detail. It comes up in disguised forms.<br /><strong>8.3 - </strong>Radix sort - Yup. It's an easy algorithm anyway.<br /><strong>8.4 - </strong>Bucket sort - can skip.<br /><br />
<h2>Chapter 9</h2>



<strong>9.1 - </strong>Small section, worth a read.<br /><strong>9.2 - </strong>Selection in expected linear time - <strong>Very </strong>important,
 as it's not common knowledge like quicksort and yet it comes up often 
in interviews. I had to code the entire thing in an interview once.<br /><strong>9.3 - </strong>Selection
 in worst-case linear time - Can skip. Just know that it's possible in 
worst-case linear time, because that might help somewhat.<br /><br />
<h2>Chapter 10</h2>



<strong>10.1 - </strong>Stacks and queues - basic knowledge, definitely very important.<br /><strong>10.2 - </strong>Linked lists - same as 10.1<br /><strong>10.3 - </strong>Implementing pointers and objects - If you use C++ or Java, skip this. Otherwise I'm not sure.<br /><strong>10.4 - </strong>Representing rooted trees - Small section, worth a quick read.<br /><br />
<h2>Chapter 11</h2>



For
 hashing, I'd say the implementation isn't as important to know as, for 
example, linked lists, but you should definitely have an idea about it 
and most importantly know the (expected and worst-case) time 
complexities of search/insert/delete etc. Also know that practically, 
they're very important data structures and, also practically, the 
expected time complexity is what matters in the real world.<br /><strong>11.1 - </strong>Direct addressing - Just understand the idea.<br /><strong>11.2 - </strong>Hash tables - important.<br /><strong>11.3 - </strong>Hash
 functions - it's worth having an idea about them, but I wouldn't go too
 in-depth here. Just know a couple examples of good and bad hash 
functions (and why they are good/bad).<br /><strong>11.4 - </strong>Open addressing - Worth having an idea about, but unlikely to come up.<br /><strong>11.5 - </strong>Perfect hashing - skip. <br /><br />
<h2>Chapter 12</h2>



<strong>12.1 - </strong>What is a binary search tree? - Yep.<br /><strong>12.2 - </strong>Querying a BST - Yep. All of it.<br /><strong>12.3 - </strong>Insertion/Deletion - Same as 12.2<br /><strong>12.4 - </strong>Randomly built BSTs - just know Theorem 12.4 (expected height of random BST is O(lgn)) and an idea of why it's true.<br /><br />
<h2>Chapter 13</h2>



This one is easy. Know what a Red-Black tree is, and what its worst-case height/insert/delete/find are. Read <strong>13.1 </strong>and <strong>13.2</strong>,
 and skip the rest. You will never be asked for RB-tree insert/delete 
unless the interviewer is "doing it wrong", or if the interviewer wants 
to see if you can re-derive the cases, in which case knowing them won't 
help much anyway (and I doubt this would happen anyway). Also know that 
RB-trees are pretty space-efficient and some C++ STL containers are 
built as RB-trees usually (e.g. map/set).<br /><br />
<h2>Chapter 14</h2>



Might be worth skimming <strong>14.2 </strong>just
 to know that you can augment data structures and why it might be 
helpful. Otherwise do one or two simple problems on augmenting data 
structures and you're set here. I'd skip <strong>14.1 </strong>and <strong>14.3</strong>.<br /><br />
<h2><strong>Chapter 15</strong></h2>



DP! Must-know.<br /><strong>15.1 - </strong>Rod-cutting. Standard DP problem, must-know.<br /><strong>15.2 - </strong>Matrix-chain
 multiplication - same as 15.1, though I don't particularly like the way
 this section is written (it's rare for me to say that about CLRS).<br /><strong>15.3 - </strong>Elements
 of DP - worth a read so that you understand DP properly, but I'd say 
it's less important than knowing what DP is (via the chapter 
introduction) and practicing on it (via the problems in this book and in
 interview preparation books).<br /><strong>15.4 - </strong>LCS - same as 15.1<br /><strong>15.5 - </strong>Optimal binary search trees - I've never read this section, so I can't argue for its importance, but I did fine without it.<br /><br />
<h2>Chapter 16</h2>



You should definitely know what a greedy algorithm is, so read the introduction for this chapter.<br /><strong>16.1 - </strong>An activity selection problem - Haven't read this in detail, but I'd say check it out, if not in-depth.<br /><strong>16.2 - </strong>Elements of the greedy strategy - same as 16.1<br /><strong>16.3 - </strong>Huffman
 codes - I'd say read the problem and the algorithm, but that's enough. 
I've seen interview questions where the answer is Huffman coding (but 
the question will come up in a 'disguised form', so it won't be 
obvious.)<br /><strong>16.4 - </strong>Matroids and greedy methods - I've never read
 this section, but I've done a lot of greedy problems during interview 
prep and this stuff never came up, so I'd say this section is irrelevant
 for the interview.<br /><strong>16.5 - </strong>Task-scheduling problem as a matroid - Same as 16.4.<br /><br />
<h2><strong>Chapter 17</strong></h2>



Okay,
 you should definitely know what amortized analysis is, but I've never 
read it from the book and I feel it's a sufficiently simple concept that
 you can just Google it and check a few examples on what it is, or 
understand it just by reading section <strong>17.1</strong>. So:<br /><strong>17.1 - </strong>Aggregate analysis - read this, it explains the important stuff.<br /><strong>17.2, 17.3, 17.4 - </strong>Skip.<br /><br />
<h2>Chapter 18</h2>



You
 should probably have an idea of what B-Trees (and B+ trees) are, I've 
heard of cases where candidates were asked about them in a general sense
 (high-level questions about what they are and why they're awesome). But
 other than that I'd skip this chapter.<br /><br />
<h2>Chapter 19</h2>



Fibonacci heaps - nope.<br /><br />
<h2>Chapter 20</h2>



van Emde Boas Trees - double, triple, and quadruple nope.<br /><br />
<h2>Chapter 21</h2>



Disjoint sets<br /><strong><span style="text-decoration: underline;">Update:</span></strong>
 I originally recommended skipping this section, but on reconsideration,
 I've noticed that it's actually more important than I originally 
thought. Thus, I recommend reading sections <strong>21.1 </strong>and <strong>21.2</strong>, while skipping the rest.<br />Union-find
 is somewhat important and I've seen at least one problem which uses it,
 though that problem could also be solved using DFS and connected 
components. That said, I also believe that it's not strictly necessary 
because one can probably, for interview purposes, come up with a similar
 enough structure easily to solve a problem which requires union-find, 
without knowing the material in this chapter. However, I believe it's 
worth a read so that if a problem comes up whose intended solution is a 
union-find data structure, you don't spend time in an interview coming 
up with it, and rather know from before, which can be a good advantage. 
Still, I'd probably rank it as less important than most of the other 
material in this list, and even less than other material that's not even
 in CLRS (like tries, for example).<br /><br />Okay, now graph algorithms. First read the introduction. Now, there's a lot to know here, so hang on.<br /><br />
<h2>Chapter 22</h2>



<strong>22.1 - </strong>Representations of graphs - Yes.<br /><strong>22.2 - </strong>BFS - Yes. After you do that, solve this problem: <a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//icpcarchive.ecs.baylor.edu/index.php%3Foption%3Dcom_onlinejudge%26Itemid%3D8%26category%3D343%26page%3Dshow_problem%26problem%3D2738" rel="nofollow noreferrer" target="_blank">ACM-ICPC Live Archive - Kermit the Frog</a>. The whole "state-space search using BFS" thing is an important concept that might be used to solve several interview problems.<br /><strong>22.3 - </strong>DFS - Yes.<br /><strong>22.4 - </strong>Topological sort - Yes.<br /><strong>22.5 - </strong>Strongly connected components - much less likely to come up than the above 4, but still possible, so: Yes.<br /><br />
<h2><strong>Chapter 23</strong></h2>



Minimum
 spanning trees - probably the least important graph algorithm, other 
than max flow (I mean for interview purposes, of course). I'd still say 
you should read it because it's such a well-known problem, but 
definitely give priority to the other things.<br /><strong>23.1 - </strong>Growing a MST - sort of, yes.<br /><strong>23.2 - </strong>Prim and Kruskal's algorithms - sort of, yes.<br /><br />
<h2>Chapter 24</h2>



Shortest path algorithms are important, though maybe less so than BFS/DFS.<br />Read
 the introduction. You should, in general, read all introductions 
anyway, but this one's important (and long), so it warranted a special 
note.<br /><strong>24.1 </strong>Bellman-Ford - Know the algorithm and its proof of correctness.<br /><strong>24.2 </strong>Shortest paths in DAGs - definitely worth knowing, may come up, even more so than Bellman-Ford I'd say.<br /><strong>24.3 </strong>Dijkstra's
 algorithm - Yes. Of course. I've seen this come up multiple times (with
 slight variations), and I've even seen A* come up.<br /><strong>24.4 </strong>Difference constraints and shortest paths - Skip.<br /><br />
<h2>Chapter 25</h2>



Read the intro as well.<br /><strong>25.1 - </strong>Matrix multiplication -I'd
 say skip. It might be possible for this to come up  (very very slim 
chance that it does though), but the chances are so low in my view that 
it's probably not worth it. If you have some extra time, though, give it
 a read.<br /><strong>25.2 - </strong>Floyd-Warshall - Yep, worth knowing the 
algorithm and its time complexity and when it works (which is for all 
weighted graphs, except ones with negative weight cycles). Its code is 
something like 5 lines so there's no reason not to know it. The analysis
 might be a bit overkill though.<br /><strong>25.3 - </strong>Johnson's algorithm - Skip.<br /><br />
<h2>Chapter 26</h2>



Maximum flow - I've never heard of this coming up in an interview and I can't imagine why it would, so skip.<br /><br />
<h2>Chapters 27+</h2>



Most
 of this stuff is never going to come up, so it's easier for me to tell 
you what to actually read than what not to read, so here are a few 
selected topics from the Selected Topics in the book:<br /><br /><strong>Chapter 31</strong><br />Most
 of what you should learn from this chapter you can learn from 
practicing on interview problems from Elements of Programming Interviews
 (and your time is better spent doing that), so I'd say skip it all 
except Euclid's algorithm for the GCD, under section <strong>31.2</strong>.<br /><br /><strong>Chapter 32</strong><br /><strong>32.1 - </strong>Naive method - just read it quickly.<br /><strong>32.2 - </strong>Rabin-Karp
 - I'd say you should know this, the rolling hash concept is very 
important and can be useful in many string- or search-related interview 
problems.<br /><br />
<h2>Appendices</h2>



<strong>A - Summations</strong><br />Know the important summations for time complexity analysis.<br /><br /><strong>C - Counting and Probability</strong><br />Give <strong>C.4</strong>
 a read if you don't know the material, Bernoulli trials may come up in 
problems (not explicitly, but you might use them, specifically for time 
analysis of questions that involve probability/coin flips).</div>